// hooks/useModernFaceDetection.ts
"use client";

import { useState, useCallback, useRef } from 'react';
import { FaceLandmarker, FilesetResolver, ImageSource } from '@mediapipe/tasks-vision';

interface FaceKeypoint {
  x: number;
  y: number;
  z?: number;
  name?: string;
}

interface MediaPipeFaceData {
  confidence: number;
  boundingBox: {
    originX: number;
    originY: number;
    width: number;
    height: number;
  };
  keypoints?: FaceKeypoint[];
  landmarks?: { x: number; y: number }[];
}

interface FaceDetectionHook {
  faceDetector: FaceLandmarker | null;
  isLoaded: boolean;
  isLoading: boolean;
  error: string | null;
  initializeDetector: () => Promise<void>;
  detectFaces: (videoElement: HTMLVideoElement) => Promise<MediaPipeFaceData[]>;
}

export function useModernFaceDetection(): FaceDetectionHook {
  const [faceDetector, setFaceDetector] = useState<FaceLandmarker | null>(null);
  const [isLoaded, setIsLoaded] = useState(false);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const detectorRef = useRef<FaceLandmarker | null>(null);

  const initializeDetector = useCallback(async () => {
    if (isLoaded || isLoading) return;

    setIsLoading(true);
    setError(null);

    try {
      console.log('üöÄ Initializing MediaPipe Face Detection...');

      // –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã MediaPipe
      const filesetResolver = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
      );

      // –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü
      const detector = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
          delegate: "GPU"
        },
        outputFaceBlendshapes: false,
        outputFacialTransformationMatrixes: false,
        runningMode: "VIDEO",
        numFaces: 1,
        minFaceDetectionConfidence: 0.7,
        minFacePresenceConfidence: 0.7,
        minTrackingConfidence: 0.7
      });

      detectorRef.current = detector;
      setFaceDetector(detector);
      setIsLoaded(true);
      console.log('‚úÖ MediaPipe Face Detection initialized successfully');

    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Unknown error occurred';
      console.error('‚ùå Failed to initialize face detection:', errorMessage);
      setError(errorMessage);
    } finally {
      setIsLoading(false);
    }
  }, [isLoaded, isLoading]);

  const detectFaces = useCallback(async (videoElement: HTMLVideoElement): Promise<MediaPipeFaceData[]> => {
    if (!detectorRef.current || !videoElement) {
      return [];
    }

    try {
      const nowInMs = Date.now();
      const results = detectorRef.current.detectForVideo(videoElement as ImageSource, nowInMs);

      if (!results || !results.faceLandmarks || results.faceLandmarks.length === 0) {
        return [];
      }

      return results.faceLandmarks.map((landmarks, index) => {
        // –ü–æ–ª—É—á–∞–µ–º bounding box –∏–∑ –ª–∞–Ω–¥–º–∞—Ä–∫–æ–≤
        const xCoords = landmarks.map(point => point.x);
        const yCoords = landmarks.map(point => point.y);
        
        const minX = Math.min(...xCoords);
        const maxX = Math.max(...xCoords);
        const minY = Math.min(...yCoords);
        const maxY = Math.max(...yCoords);

        // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –ø–∏–∫—Å–µ–ª–∏
        const videoWidth = videoElement.videoWidth;
        const videoHeight = videoElement.videoHeight;

        const boundingBox = {
          originX: minX * videoWidth,
          originY: minY * videoHeight,
          width: (maxX - minX) * videoWidth,
          height: (maxY - minY) * videoHeight
        };

        // –ö–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –ª–∏—Ü–∞ (–≥–ª–∞–∑–∞, –Ω–æ—Å, —Ä–æ—Ç)
        const keypoints: FaceKeypoint[] = [
          // –õ–µ–≤—ã–π –≥–ª–∞–∑ (–ª–∞–Ω–¥–º–∞—Ä–∫ 33)
          { x: landmarks[33].x, y: landmarks[33].y, name: 'left_eye' },
          // –ü—Ä–∞–≤—ã–π –≥–ª–∞–∑ (–ª–∞–Ω–¥–º–∞—Ä–∫ 263)
          { x: landmarks[263].x, y: landmarks[263].y, name: 'right_eye' },
          // –ö–æ–Ω—á–∏–∫ –Ω–æ—Å–∞ (–ª–∞–Ω–¥–º–∞—Ä–∫ 1)
          { x: landmarks[1].x, y: landmarks[1].y, name: 'nose_tip' },
          // –õ–µ–≤—ã–π —É–≥–æ–ª–æ–∫ —Ä—Ç–∞ (–ª–∞–Ω–¥–º–∞—Ä–∫ 61)
          { x: landmarks[61].x, y: landmarks[61].y, name: 'mouth_left' },
          // –ü—Ä–∞–≤—ã–π —É–≥–æ–ª–æ–∫ —Ä—Ç–∞ (–ª–∞–Ω–¥–º–∞—Ä–∫ 291)
          { x: landmarks[291].x, y: landmarks[291].y, name: 'mouth_right' }
        ];

        return {
          confidence: 0.95, // MediaPipe –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç confidence –Ω–∞–ø—Ä—è–º—É—é, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
          boundingBox,
          keypoints,
          landmarks: landmarks.map(point => ({ x: point.x, y: point.y }))
        };
      });

    } catch (err) {
      console.error('‚ùå Face detection error:', err);
      return [];
    }
  }, []);

  return {
    faceDetector,
    isLoaded,
    isLoading,
    error,
    initializeDetector,
    detectFaces
  };
}

// –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–∏–ø—ã
export type { MediaPipeFaceData, FaceKeypoint, FaceDetectionHook };
